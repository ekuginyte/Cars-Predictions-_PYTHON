{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID5059 - Knowledge Discovery & Data Mining\n",
    "# Coursework Assignment 1 - Individual\n",
    "# Deadline: Friday 24th February 2023 (week 6), 9pm\n",
    "# Student: Erna Kuginyte 220013309\n",
    "\n",
    "################################################################################\n",
    "###################### IMPORT LIBRARIES, DATA FRAME ############################\n",
    "################################################################################\n",
    "\n",
    "# Load libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from zlib import crc32\n",
    "# Correlation matrix plots\n",
    "from pandas.plotting import scatter_matrix\n",
    "# Import to change the character variables to categorical\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "# Import to change the character variables to numerical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# This line runs on Jupyter Notebook only\n",
    "#get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n",
    "\n",
    "# Read the dataset from a file\n",
    "# Specify type \"string\" for the \"bed\" and \"dealer_zip\" attributes to avoid \n",
    "#       errors interpreting them as numbers\n",
    "cars   =   pd.read_csv(\"/Users/ernakuginyte/Desktop/Projects for Applications/Cars-Predictions-_PYTHON/data/used_cars_data_medium_1.csv\", \\\n",
    "    dtype   =   {\"bed\": \"string\", \"dealer_zip\": \"string\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "############################# EXPLORE THE DATA #################################\n",
    "################################################################################\n",
    "\n",
    "# Clear the maximum number of columns to be displayed, so that all will be visible\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Check the basic statistics of the data\n",
    "cars.describe()\n",
    "# Display the data frame\n",
    "print(cars)\n",
    "# Explore numeric columns visually\n",
    "cars.hist(bins = 20, figsize = (20, 15)) \n",
    "\n",
    "########################### EXPLORE THE PRICE VARIABLE ##########################\n",
    "\n",
    "# Inspect the price variable by plotting a histogram with a curve on top.\n",
    "# Explore numeric columns visually.\n",
    "# Set the figure size\n",
    "plt.figure(figsize = (10, 8))\n",
    "# Plot the histogram with the kde curve \n",
    "sns.distplot(cars[\"price\"], kde = True, bins = 25, color = \"firebrick\", \\\n",
    "             hist_kws = {\"alpha\": 0.2, \"edgecolor\": \"tomato\", \"linewidth\": 1.5}, \\\n",
    "                kde_kws = {\"linewidth\": 2.5})\n",
    "# Set the title and axis labels)\n",
    "# Set the title and axis labels\n",
    "plt.title(\"Distribution of Car Prices\", fontsize = 16)\n",
    "plt.xlabel(\"Price\", fontsize = 12)\n",
    "plt.ylabel(\"Density\", fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "############################# DATA WRANGLING ###################################\n",
    "################################################################################\n",
    "\n",
    "################################ DUPLICATES ####################################\n",
    "\n",
    "# Firstly, check for duplicates\n",
    "print(cars.duplicated().sum()) \n",
    "# Drop duplicates\n",
    "cars = cars.drop_duplicates()\n",
    "\n",
    "########################## DEAL WITH MISSING VALUES ############################\n",
    "\n",
    "# Deal with the missing values.\n",
    "# Check which columns have only missing values and drop them.\n",
    "cars = cars.dropna(axis = 1, how = \"all\")\n",
    "\n",
    "# Add 0 to \"owner_count\" in missing values if \"is_new\" is True\n",
    "cars.loc[(cars[\"is_new\"] == True) & (cars[\"owner_count\"].isnull()), \"owner_count\"] = 0\n",
    "\n",
    "# Check which columns have some missing values.\n",
    "# First find the number of missing values in each column.\n",
    "na_columns = cars.isna().sum()\n",
    "# Save the column names with missing values\n",
    "cols_with_na = list(na_columns[na_columns > 0].index)\n",
    "# Print the column names with number of missing values \n",
    "#    out of total number of rows in the data set\n",
    "for col in cols_with_na:\n",
    "    print(f\"{col}: {na_columns[col]} missing values out of {cars.shape[0]}\")\n",
    "\n",
    "### clean_df - Cleans dataframe columns with missing data;\n",
    "###            if more than 25% of rows are missing, the whole column will be dropped;\n",
    "###            else, add median numeric values to the numeric data, string or object data won\"t be modified.  \n",
    "#   INPUT:\n",
    "#              df - data frame with latent values.\n",
    "#   OUTPUT:\n",
    "#              df_cleaned - cleaned data frame.\n",
    "def clean_df(df):\n",
    "    \n",
    "    # Firstly check if the input value is of correct type\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\"Input argument incorrect, it should be a Data Frame! :)\")\n",
    "    # Check if the data frame has any missing values\n",
    "    if not df.isnull().values.any():\n",
    "        raise ValueError(\"Your data frame does not have any missing values, hooray! :)\")    \n",
    "        \n",
    "    # Find the number of missing values in each column\n",
    "    na_columns = df.isna().sum()\n",
    "\n",
    "    # Calculate the percentage of missing values in each column\n",
    "    na_columns_percent = na_columns / df.shape[0]\n",
    "\n",
    "    # Get the names of the columns with more than 25% missing values\n",
    "    cols_to_drop = list(na_columns_percent[na_columns_percent > 0.25].index)\n",
    "\n",
    "    # Drop the columns with more than 25% missing values\n",
    "    df_cleaned = df.drop(cols_to_drop, axis = 1)\n",
    "\n",
    "    # Replace the missing values in the remaining columns with the median value\n",
    "    # if the column is numeric, otherwise leave the values as is\n",
    "    for col in df_cleaned.columns:\n",
    "        # First check if the data type is numeric\n",
    "        if df_cleaned[col].dtype in [\"float64\", \"int64\"]:\n",
    "            # Fill the missing values with medians\n",
    "            df_cleaned[col].fillna(df_cleaned[col].median(), inplace = True)\n",
    "            \n",
    "    # Return the cleaned data frame\n",
    "    return df_cleaned\n",
    "\n",
    "# Apply the function and save the cleaned data frame\n",
    "cars = clean_df(df = cars)\n",
    "# Print names of non-numeric columns with the number of missing values.\n",
    "# Get the number of missing values in each non-numeric column\n",
    "na_string_columns = cars.isna().sum()\n",
    "\n",
    "# Get the names of non-numeric columns with missing values\n",
    "cols_with_na = list(na_string_columns[na_string_columns > 0].index)\n",
    "\n",
    "# Print the names of non-numeric columns with the number of missing values\n",
    "for col in cols_with_na:\n",
    "    print(f\"{col}: {na_string_columns[col]} missing values out of {cars.shape[0]}\")\n",
    "\n",
    "####################### ADD MEAN TO THE MISSING VALUES #########################\n",
    "\n",
    "### fill_na_mean - Take away the \" in\", \"in\", \" gal\" measurement units from data set \n",
    "###                variables and converts it to a numeric object;\n",
    "###                Fills latent variables with mean value.\n",
    "#   INPUT:  \n",
    "#                  variables_inches - names of variables that have \" in\" inches measurement;\n",
    "#                                     also has some missing values.\n",
    "#   OUTPUT: \n",
    "#                  data - cleaned data frame.\n",
    "def fill_na_mean(data, vars):\n",
    "\n",
    "    # Firstly check if the input value is of correct type.\n",
    "    # Check if data is a data frame.\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        raise TypeError(\"Input argument incorrect, it should be a Data Frame! :)\")\n",
    "    # Check if the data frame has any missing values\n",
    "    if not data.isnull().values.any():\n",
    "        raise ValueError(\"Your data frame does not have any missing values, hooray! :)\")\n",
    "    # Check if the variables_inches vector is not empty\n",
    "    if len(vars) == 0:\n",
    "        raise ValueError(\"Variable list is empty! :)\")\n",
    "\n",
    "        # For each variable in the variables_inches list\n",
    "    for variable in vars:\n",
    "\n",
    "        # Check if the variable type is object\n",
    "        if data[variable].dtype == object:\n",
    "            # Remove the \"in\" and \"gal\" from the values\n",
    "            data[variable] = data[variable].str.replace(\" in\", \"\").str.replace(\" gal\", \"\").\\\n",
    "                str.replace(\"in\", \"\").str.replace(\"--\", \"\")\n",
    "            # Convert the string variables to numeric\n",
    "            data[variable] = pd.to_numeric(data[variable], errors = \"coerce\")\n",
    "        # Fill missing values with the mean\n",
    "        data[variable] = data[variable].fillna(data[variable].mean())\n",
    "\n",
    "    # Return the wrangled data frame\n",
    "    return data\n",
    "\n",
    "# Create a list of variables to fill the NAs with mean\n",
    "variables_inches = [\"back_legroom\", \"front_legroom\", \"height\", \"length\", \\\n",
    "    \"wheelbase\", \"width\", \"fuel_tank_volume\"]\n",
    "\n",
    "# Apply the fill_na_mean function and save the cleaned data frame\n",
    "cars = fill_na_mean(data = cars, vars = variables_inches)\n",
    "\n",
    "# The variables that have too many missing values or cannot be filled manually\n",
    "#   as they are character variables; main_picture_url useless for this analysis\n",
    "columns_to_drop = [\"engine_cylinders\", \"fuel_type\", \"main_picture_url\", \"trimId\", \\\n",
    "    \"major_options\", \"power\", \"torque\", \"transmission_display\", \"trim_name\", \\\n",
    "        \"wheel_system\", \"wheel_system_display\", \"main_picture_url\", \"description\", \\\n",
    "            \"maximum_seating\", \"engine_type\", \"vin\", \"body_type\"]\n",
    "# Drop the unwanted variables\n",
    "cars.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "\n",
    "########################## FILL OTHER MISSING VALUES ###########################\n",
    "\n",
    "# Deal with missing values in listing_color and exterior_color variables.\n",
    "# Replace the \"UNKNOWN\" values in listing_color with values from exterior_color.\n",
    "cars.loc[cars[\"listing_color\"] == \"UNKNOWN\", \"listing_color\"] = cars[\"exterior_color\"]\n",
    "# Replace the \"None\" values in exterior_color with values in listing_color\n",
    "cars.loc[cars[\"exterior_color\"] == \"None\", \"exterior_color\"] = cars[\"listing_color\"]\n",
    "# Delete rows where listing_color is UNKNOWN and exterior_color is None\n",
    "cars = cars.loc[(cars[\"listing_color\"] != \"UNKNOWN\") | (cars[\"exterior_color\"].notna())]\n",
    "\n",
    "# Calculate the mean of the \"owner_count\" column\n",
    "mean_owner_count = np.nanmean(cars[\"owner_count\"])\n",
    "# Round the mean to the nearest integer\n",
    "rounded_mean_owner_count = int(round(mean_owner_count))\n",
    "# Replace missing values with the rounded mean\n",
    "cars[\"owner_count\"] = cars[\"owner_count\"].fillna(rounded_mean_owner_count)\n",
    "\n",
    "# Check which variables have missing values\n",
    "print(cars.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "###################### DEAL WITH OTHER OBJECT VARIABLES ########################\n",
    "################################################################################\n",
    "\n",
    "################################ CATEGORICAL ###################################\n",
    "# Convert categories from text to numbers\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "# Define which variables to change to categorical\n",
    "char_variables = [\"city\", \"franchise_dealer\", \\\n",
    "    \"is_new\", \"listing_color\", \"make_name\", \"model_name\", \\\n",
    "        \"sp_name\", \"transmission\", \"dealer_zip\", \"year\"]\n",
    "# Categorise variables\n",
    "for variables in char_variables:\n",
    "    cars[variables] = cars[variables].astype(\"category\")\n",
    "\n",
    "################################# NUMERICAL #####################################\n",
    "# Encode character variables to numerical ones\n",
    "# Create a label encoder object\n",
    "encoder = LabelEncoder()\n",
    "# Variables to convert to numerical ones\n",
    "char_to_num_variables = [\"exterior_color\", \"city\", \"interior_color\", \"make_name\", \\\n",
    "    \"model_name\", \"sp_name\"]\n",
    "# Convert the character columns to numerical variables using transform function\n",
    "for var in char_to_num_variables:\n",
    "    encoder.fit(cars[var])\n",
    "    cars[var+\"_code\"] = encoder.transform(cars[var].astype(str)) \n",
    "\n",
    "#################################### DATE ########################################\n",
    "\n",
    "# Convert listed_date from object to Unix timestamp\n",
    "cars[\"listed_date\"] = pd.to_datetime(cars[\"listed_date\"]).astype(int) // 10 ** 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "############################## ADD EXTRA VARIABLES #############################\n",
    "################################################################################\n",
    "\n",
    "# Add an extra variable savings_per_day\n",
    "cars[\"savings_per_day\"] = np.where(cars[\"daysonmarket\"] == 0, 0, \\\n",
    "    cars[\"savings_amount\"] / cars[\"daysonmarket\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################ CORRELATION ###################################\n",
    "################################################################################\n",
    "\n",
    "# Check which columns will be taken in for correlation function\n",
    "print(cars.dtypes)\n",
    "# Check for correlation in the data.\n",
    "# Calculate the Pearson correlation coefficients for all covariates.\n",
    "cor = cars.corr()\n",
    "# Print the correlation coefficients\n",
    "print(cor)\n",
    "\n",
    "### custom_annotave - Custom annotating function.\n",
    "#   INPUT:\n",
    "#                     value - value of correlation to be highlighted;\n",
    "#                     symbol - symbol to be used to highlight the values.\n",
    "#   OUTPUT: \n",
    "#                     \n",
    "def custom_annotate(value, symbol = \"+\"):\n",
    "    if abs(value) >= 0.75:\n",
    "        return symbol\n",
    "    return \"\"\n",
    "\n",
    "# Apply the custom annotating function to correlation data\n",
    "annot = np.vectorize(custom_annotate)(cor)\n",
    "\n",
    "# Plot the heatmap of correlations between variables.\n",
    "# Plot size.\n",
    "plt.figure(figsize = (10, 7))\n",
    "# Plot the heatmap \n",
    "sns.heatmap(cor, annot = annot, cmap = \"YlGnBu\", linewidths = 0.5, fmt = \"\")\n",
    "# Set heatmap title\n",
    "plt.title(\"Correlation Graph\", c = \"r\", size = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "####################### SELECT VARIABLES FOR THE MODEL #########################\n",
    "################################################################################\n",
    "\n",
    "# First visually explore the relationship between the variables and the \"price\" \n",
    "\n",
    "### important_variables - Function to select variables for the model.\n",
    "#   INPUT:  \n",
    "#                         corr_threshold - threshold of correlation with the price;\n",
    "#                         data - data frame;\n",
    "#   OUTPUT:\n",
    "#                         cars_selected - updated data frame.\n",
    "def important_variables(data = cars, corr_threshold = 0.2):\n",
    "    \n",
    "    # Firstly check if the input value is of correct type\n",
    "    # Check if data is a data frame\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        raise TypeError(\"Input argument incorrect, it should be a Data Frame! :)\")\n",
    "    # Check if the data frame has more than 1 column\n",
    "    if len(data.columns) < 2:\n",
    "        raise ValueError(\"Your data frame has only 1 attribute! :O\")\n",
    "    # Check if the threshold value is numeric and its length is 1\n",
    "    if not isinstance(corr_threshold, (int, float)) and len(str(corr_threshold))!= 1:\n",
    "        raise TypeError(\"Input argument incorrect, it should be a numeric value! :)\")\n",
    "    # Check if the threshold value is greater than 0\n",
    "    if corr_threshold <= 0:\n",
    "        raise ValueError(\"Input argument incorrect, it should be a positive numeric value! :)\")\n",
    "\n",
    "    # Get correlation matrix of all pairs of variables\n",
    "    corr_matrix = data.corr()[\"price\"]\n",
    "    # Pre-set a variable to store selection\n",
    "    selected_vars = []\n",
    "    cars_selected = pd.DataFrame()\n",
    "    # Select the variables with correlation greater than or equal to the threshold\n",
    "    for col in corr_matrix.index:\n",
    "        if abs(corr_matrix[col]) >= corr_threshold and col != \"price\":\n",
    "            selected_vars.append(col)\n",
    "\n",
    "    # Add the prediction variable \"price\" to the selected variables and create a new data frame\n",
    "    selected_vars.append(\"price\")\n",
    "    cars_selected = data[selected_vars]\n",
    "    # Return the new data frame\n",
    "    return cars_selected\n",
    "\n",
    "# Get the new data frame with selected variables\n",
    "cars_selected = important_variables(data = cars, corr_threshold = 0.15)\n",
    "\n",
    "# Get correlation matrix of all pairs of variables\n",
    "corr_matrix = cars_selected.corr()\n",
    "# Plot heatmap of correlation.\n",
    "# Plot size.\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(corr_matrix, annot = False, cmap = \"YlGnBu\", linewidths = 0.5, fmt = \"+\")\n",
    "# Set heatmap title\n",
    "plt.title(\"Correlation Graph\", c = \"r\", size = 25)\n",
    "\n",
    "\n",
    "# Get correlation matrix of selected variables and price\n",
    "corr_matrix_price = cars_selected.corr()[\"price\"]\n",
    "# Print the matrix\n",
    "corr_matrix_price.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "###################### TRAINING AND TEST SETS SPLIT ############################\n",
    "################################################################################\n",
    "\n",
    "# To split the training from testing sections\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Check quantiles to then categorise the data in to 5 sections\n",
    "cat = cars_selected[\"price\"].describe()\n",
    "\n",
    "# Create price_categ column to split the data into evenly distributed\n",
    "# training and testing sets\n",
    "# Split the data by \"price\" column into normally distributed 5 bins\n",
    "cars_selected[\"price_categ\"] = pd.qcut(cars_selected[\"price\"], q = 5, labels = [1, 2, 3, 4, 5])\n",
    "# Plot the category values in a histogram to check if it makes sense\n",
    "cars_selected[\"price_categ\"].hist()\n",
    "\n",
    "# Split the training and the testing sets\n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 7) # 7 being the lucky number\n",
    "for train_index, test_index in split.split(cars_selected, cars_selected[\"price_categ\"]):\n",
    "    strat_train_set = cars_selected.loc[train_index]\n",
    "    strat_test_set = cars_selected.loc[test_index]\n",
    "\n",
    "# Check if the categorical split makes sense\n",
    "price_distribution = strat_test_set[\"price\"].value_counts() / len(strat_test_set)\n",
    "# Plot the histogram of the price distribution\n",
    "price_distribution.hist()\n",
    "\n",
    "# Check the proportions of each price category in the training and testing sets\n",
    "print(strat_train_set[\"price_categ\"].value_counts(normalize = True))\n",
    "print(strat_test_set[\"price_categ\"].value_counts(normalize =True))\n",
    "\n",
    "# Plot histograms of price_categ in the original data, training, and testing sets\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 3, figsize = (12, 4))\n",
    "cars_selected[\"price_categ\"].hist(ax = ax[0], bins = 5, color = \"orange\", edgecolor = \"darkgoldenrod\")\n",
    "ax[0].set_title(\"Original Data\")\n",
    "ax[0].set_ylim([5500, 5650])\n",
    "strat_train_set[\"price_categ\"].hist(ax = ax[1], bins = 5, color = \"orange\", edgecolor = \"darkgoldenrod\")\n",
    "ax[1].set_title(\"Training Set\")\n",
    "ax[1].set_ylim([4400, 4500])\n",
    "strat_test_set[\"price_categ\"].hist(ax = ax[2], bins = 5, color = \"orange\", edgecolor = \"darkgoldenrod\")\n",
    "ax[2].set_title(\"Testing Set\")\n",
    "ax[2].set_ylim([1100, 1130])\n",
    "# Overall subtitle\n",
    "fig.suptitle(\"Distribution of Price Categories\", fontsize = 14)\n",
    "\n",
    "# Drop the category column as it should not predict anything in the model\n",
    "for set_ in (strat_train_set, strat_test_set): \n",
    "    set_.drop(\"price_categ\", axis = 1, inplace = True)\n",
    "\n",
    "# Split the predictors from the target\n",
    "# Train set\n",
    "X_train = strat_train_set.copy()\n",
    "y_train = strat_train_set[\"price\"].copy()\n",
    "# Test set\n",
    "X_test = strat_test_set.copy()\n",
    "y_test = strat_test_set[\"price\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "############################## FEATURE SCALING #################################\n",
    "################################################################################\n",
    "\n",
    "# Import the standard scaler and pipeline functions\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Import the function to fill in any missing numeric values\n",
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "# Define the NA's imputer\n",
    "imputer = SimpleImputer(strategy = \"median\")\n",
    "# Build pipeline to scale the data set\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", imputer),\n",
    "    (\"std_scaler\", StandardScaler()),\n",
    "    ])\n",
    "\n",
    "# Import the column transformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# Select the columns to be scaled\n",
    "num_cols = strat_train_set.select_dtypes(include = np.number).columns.tolist()\n",
    "# Column transformer pipeline, full\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "########################## LINEAR REGRESSION MODEL #############################\n",
    "################################################################################\n",
    "\n",
    "###################### FIT AND TRANSFORM THE DATA SETS #########################\n",
    "\n",
    "# Transform the training and test sets\n",
    "X_train_transformed = full_pipeline.fit_transform(X_train)\n",
    "X_test_transformed = full_pipeline.transform(X_test)\n",
    "\n",
    "# Import the linear regression functions\n",
    "from sklearn.linear_model import LinearRegression \n",
    "# Save the linear regression function\n",
    "lin_reg = LinearRegression()\n",
    "# Fit the first linear regression model\n",
    "lin_reg.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Check predictions for the first 5 rows in the training set\n",
    "print(\"Training set predictions:\")\n",
    "print(lin_reg.predict(X_train_transformed[:5]))\n",
    "# Check predictions for the first 5 rows in the test set\n",
    "print(\"Test set predictions:\")\n",
    "print(lin_reg.predict(X_test_transformed[:5]))\n",
    "# Seems the regression predictions are working, now we need to evaluate the model\n",
    "\n",
    "############################# EVALUATE THE MODEL ###############################\n",
    "\n",
    "### R-SQUARED SCORE\n",
    "\n",
    "# Calculate R-squared score for the trained model on the test dataset\n",
    "y_test_pred = lin_reg.predict(X_test_transformed)\n",
    "# R-squared metric to check the model performance\n",
    "from sklearn.metrics import r2_score\n",
    "# Calculate the R-squared score\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "# Print the score\n",
    "print(\"R-squared model 1:\", r2)\n",
    "\n",
    "### RMSE\n",
    "\n",
    "# Import the MSE function\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Compute the MSE\n",
    "lin_mse = mean_squared_error(y_test, y_test_pred) \n",
    "# Compute the RMSE\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "# Print the score\n",
    "print(\"RMSE model 1:\", lin_rmse)\n",
    "\n",
    "### MAE\n",
    "\n",
    "# Import the MAE function\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Print MAE score\n",
    "print(\"MAE model 1: \", mean_absolute_error(y_test, y_test_pred))\n",
    "\n",
    "### CROSS VALIDATION \n",
    "\n",
    "# Import the cross validation function\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform 10-fold cross-validation on the linear regression model\n",
    "scores = cross_val_score(lin_reg, X_train_transformed, y_train, \\\n",
    "                         cv = 10, scoring = \"neg_mean_squared_error\")\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "# Define display cv scores\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "# Print the cross-validation scores and their mean\n",
    "display_scores(tree_rmse_scores)\n",
    "\n",
    "### AIC\n",
    "\n",
    "# Import the AIC function\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Fit the linear regression model using statsmodels function\n",
    "# Add the constant term for intercept\n",
    "X_train_sm = sm.add_constant(X_train_transformed)  \n",
    "lin_reg_sm = sm.OLS(y_train, X_train_sm)\n",
    "lin_reg_sm_fit = lin_reg_sm.fit()\n",
    "\n",
    "# Print the AIC score\n",
    "print(\"AIC model 1:\", lin_reg_sm_fit.aic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## REDUCE MODEL COMPLEXITY #############################\n",
    "\n",
    "# Transform the training and test sets\n",
    "X_train_transformed = full_pipeline.fit_transform(X_train)\n",
    "X_test_transformed = full_pipeline.transform(X_test)\n",
    "\n",
    "# Import the function to reduce the complexity of the model\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Fit a linear regression model to obtain feature importances\n",
    "lin_reg.fit(X_train_transformed, y_train)\n",
    "importances = lin_reg.coef_\n",
    "\n",
    "# Select features based on their importance score\n",
    "selector = SelectFromModel(estimator=lin_reg, threshold = \"median\")\n",
    "X_train_selected = selector.fit_transform(X_train_transformed, y_train)\n",
    "X_test_selected = selector.transform(X_test_transformed)\n",
    "# Train a new model on the selected features\n",
    "lin_reg_selected = LinearRegression()\n",
    "lin_reg_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the performance of the new model\n",
    "y_pred_selected = lin_reg_selected.predict(X_test_selected)\n",
    "r2_selected = r2_score(y_test, y_pred_selected)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features = selector.get_support(indices = True)\n",
    "feature_names = list(strat_train_set.columns[selected_features])\n",
    "\n",
    "###################### FIT AND TRANSFORM THE DATA SETS #########################\n",
    "### Fit the new training set only using the selected variables\n",
    "\n",
    "# Reconstruct the pipeline (FOR SOME REASON NUM_COLS = FEATURE_NAMES DID NOT WORK??)\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, feature_names)\n",
    "])\n",
    "\n",
    "# Train set\n",
    "X_train = strat_train_set[num_cols].copy()\n",
    "y_train = strat_train_set[\"price\"].copy()\n",
    "# Test set\n",
    "X_test = strat_test_set[num_cols].copy()\n",
    "y_test = strat_test_set[\"price\"].copy()\n",
    "\n",
    "# Transform the training and test sets\n",
    "X_train_transformed = full_pipeline.fit_transform(X_train)\n",
    "X_test_transformed = full_pipeline.transform(X_test)\n",
    "# Fit the updated linear regression model\n",
    "lin_reg.fit(X_train_transformed, y_train)\n",
    "\n",
    "############################# EVALUATE THE MODEL ###############################\n",
    "\n",
    "### R-SQUARED SCORE\n",
    "\n",
    "# Calculate R-squared score for the trained model on the test dataset\n",
    "y_test_pred = lin_reg.predict(X_test_transformed)\n",
    "# Calculate the R-squared score\n",
    "r2_selected = r2_score(y_test, y_test_pred)\n",
    "# Print the score\n",
    "print(\"R-squared model 2:\", r2_selected) # Score: 1.0\n",
    "\n",
    "### RMSE\n",
    "\n",
    "# Extract predictions from the training set\n",
    "price_predictions = lin_reg.predict(X_train_transformed)\n",
    "# Compute the MSE\n",
    "lin_mse = mean_squared_error(y_train, price_predictions) \n",
    "# Compute the RMSE\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "# Print the score\n",
    "print(\"RMSE model 2:\", lin_rmse)\n",
    "\n",
    "### MAE\n",
    "\n",
    "# Print MAE score\n",
    "print(\"MAE model 2: \", mean_absolute_error(y_train, price_predictions))\n",
    "\n",
    "### AIC\n",
    "\n",
    "# Fit the linear regression model using statsmodels function\n",
    "# Add the constant term for intercept\n",
    "X_train_sm = sm.add_constant(X_train_transformed)  \n",
    "lin_reg_sm = sm.OLS(y_train, X_train_sm)\n",
    "lin_reg_sm_fit = lin_reg_sm.fit()\n",
    "\n",
    "# Print the AIC score\n",
    "print(\"AIC model 2:\", lin_reg_sm_fit.aic)\n",
    "\n",
    "### VISUAL EVALUATION\n",
    "\n",
    "# Plot given prices against predicted prices\n",
    "plt.scatter(y_test, y_test_pred)\n",
    "# Plot the line\n",
    "plt.plot([0, np.max(y_test)], [0, np.max(y_test)], \"k--\", lw = 3)\n",
    "# Labels\n",
    "plt.xlabel(\"Actual Price\")\n",
    "plt.ylabel(\"Predicted Price\")\n",
    "plt.title(\"Actual Prices vs Predicted Prices\")\n",
    "\n",
    "### CROSS VALIDATION SCORE\n",
    "\n",
    "# Perform 10-fold cross-validation on the linear regression model\n",
    "scores = cross_val_score(lin_reg, X_train_transformed, y_train, \\\n",
    "                         cv = 10, scoring = \"neg_mean_squared_error\")\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "# Define display cv scores\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "# Print the cross-validation scores and their mean\n",
    "display_scores(rmse_scores)\n",
    "\n",
    "### VIF\n",
    "\n",
    "### Check variance inflation factor\n",
    "# Import the functions for VIF\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Create a dataframe to store the VIFs \n",
    "vif_df = pd.DataFrame(columns = [\"predictor\", \"VIF\"])\n",
    "\n",
    "# Convert X_train to a DataFrame\n",
    "X_train_df = pd.DataFrame(X_train_transformed, columns = feature_names)\n",
    "\n",
    "# Calculate the VIF for each feature\n",
    "for i in range(len(feature_names)):\n",
    "    vif = variance_inflation_factor(X_train_df[feature_names].values, i)\n",
    "    feature_name = feature_names[i]\n",
    "    vif_df.loc[i] = [feature_name, vif]\n",
    "\n",
    "# Print the VIF dataframe\n",
    "print(vif_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "############################## RIDGE REGRESSION ################################\n",
    "################################################################################\n",
    "\n",
    "# Train set\n",
    "X_train = strat_train_set.copy()\n",
    "y_train = strat_train_set[\"price\"].copy()\n",
    "# Test set\n",
    "X_test = strat_test_set.copy()\n",
    "y_test = strat_test_set[\"price\"].copy()\n",
    "\n",
    "# Rewrite the pipeline\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols)\n",
    "])\n",
    "\n",
    "# Transform the training and test sets\n",
    "X_train_transformed = full_pipeline.fit_transform(X_train)\n",
    "X_test_transformed = full_pipeline.transform(X_test)\n",
    "\n",
    "# Import Ridge function \n",
    "from sklearn.linear_model import Ridge\n",
    "# Import grid search to find the optimal alpha value\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define the Ridge regression model\n",
    "ridge_reg = Ridge()\n",
    "\n",
    "################################ GRID SEARCH ##################################\n",
    "\n",
    "# Grid values\n",
    "param_grid = {\"alpha\": np.arange(0.001, 1.02, 0.002)}\n",
    "grid_search = GridSearchCV(ridge_reg, param_grid, cv = 10)\n",
    "# Fit the grid\n",
    "grid_search.fit(X_train_transformed, y_train)\n",
    "# Print the best alpha value\n",
    "print(\"Best alpha value: \", grid_search.best_params_[\"alpha\"])\n",
    "\n",
    "############################### FIT BEST ALPHA #################################\n",
    "\n",
    "# Define the Ridge regression model\n",
    "ridge_reg = Ridge(alpha = grid_search.best_params_[\"alpha\"])\n",
    "# Fit the pipeline to the training data\n",
    "ridge_reg.fit(X_train_transformed, y_train)\n",
    "# Ridge model predictions\n",
    "ridge_y_pred = ridge_reg.predict(X_test_transformed)\n",
    "\n",
    "############################# EVALUATE THE MODEL ###############################\n",
    "\n",
    "# R2, RMSE, MAE\n",
    "print(\"R2 score Ridge model: \", r2_score(y_test, ridge_y_pred))\n",
    "print(\"RMSE Ridge model: \", np.sqrt(mean_squared_error(y_test, ridge_y_pred)))\n",
    "print(\"MAE Ridge model: \", mean_absolute_error(y_test, ridge_y_pred))\n",
    "# Perform 10-fold cross-validation on the linear ridge model\n",
    "scores = cross_val_score(ridge_reg, X_train_transformed, y_train, \\\n",
    "                         cv = 10, scoring = \"neg_mean_squared_error\")\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "# Define display cv scores\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "# Print the cross-validation scores and their mean\n",
    "display_scores(rmse_scores)\n",
    "\n",
    "# AIC\n",
    "# Get the number of features in the model\n",
    "num_feat = X_train_transformed.shape[1]\n",
    "# Calculate the residual sum of squares\n",
    "rss = np.sum((y_train - ridge_reg.predict(X_train_transformed)) ** 2)\n",
    "# Calculate the AIC score\n",
    "aic = 2 * num_feat + len(y_train) * np.log(rss / len(y_train))\n",
    "# Print AIC score\n",
    "print(\"AIC score Ridge model: \", aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "############################## LASSO REGRESSION ################################\n",
    "################################################################################\n",
    "\n",
    "# Train set\n",
    "X_train = strat_train_set.copy()\n",
    "y_train = strat_train_set[\"price\"].copy()\n",
    "# Test set\n",
    "X_test = strat_test_set.copy()\n",
    "y_test = strat_test_set[\"price\"].copy()\n",
    "\n",
    "# Rewrite the pipeline\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols)\n",
    "])\n",
    "\n",
    "# Transform the training and test sets\n",
    "X_train_transformed = full_pipeline.fit_transform(X_train)\n",
    "X_test_transformed = full_pipeline.transform(X_test)\n",
    "\n",
    "# Import Lasso function \n",
    "from sklearn.linear_model import Lasso\n",
    "# Define the Lasso regression model\n",
    "lasso_reg = Lasso()\n",
    "\n",
    "################################ GRID SEARCH ##################################\n",
    "\n",
    "# Grid values\n",
    "param_grid = {\"alpha\": np.arange(0.001, 1.02, 0.002)}\n",
    "grid_search = GridSearchCV(lasso_reg, param_grid, cv = 10)\n",
    "# Fit the grid\n",
    "grid_search.fit(X_train_transformed, y_train)\n",
    "# Print the best alpha value\n",
    "print(\"Best alpha value: \", grid_search.best_params_[\"alpha\"])\n",
    "\n",
    "############################### FIT BEST ALPHA #################################\n",
    "\n",
    "# Define the Lasso regression model\n",
    "lasso_reg = Lasso(alpha = grid_search.best_params_[\"alpha\"])\n",
    "# Fit the pipeline to the training data\n",
    "lasso_reg.fit(X_train_transformed, y_train)\n",
    "# Lasso model predictions\n",
    "lasso_y_pred = lasso_reg.predict(X_test_transformed)\n",
    "\n",
    "############################# EVALUATE THE MODEL ###############################\n",
    "\n",
    "# R2, RMSE, MAE\n",
    "print(\"R2 score Lasso model: \", r2_score(y_test, lasso_y_pred))\n",
    "print(\"RMSE Lasso model: \", np.sqrt(mean_squared_error(y_test, lasso_y_pred)))\n",
    "print(\"MAE Lasso model: \", mean_absolute_error(y_test, lasso_y_pred))\n",
    "\n",
    "# Perform 10-fold cross-validation on the linear regression model\n",
    "scores = cross_val_score(lasso_reg, X_train_transformed, y_train, \\\n",
    "                         cv = 10, scoring = \"neg_mean_squared_error\")\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "# Define display cv scores\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "# Print the cross-validation scores and their mean\n",
    "display_scores(rmse_scores)\n",
    "\n",
    "# AIC\n",
    "# Get the number of features in the model\n",
    "num_feat = X_train_transformed.shape[1]\n",
    "# Calculate the residual sum of squares\n",
    "rss = np.sum((y_train - lasso_reg.predict(X_train_transformed)) ** 2)\n",
    "# Calculate the AIC score\n",
    "aic = 2 * num_feat + len(y_train) * np.log(rss / len(y_train))\n",
    "# Print AIC score\n",
    "print(\"AIC score Lasso model: \", aic)\n",
    "\n",
    "### VIFs\n",
    "\n",
    "# Create a dataframe to store the VIFs \n",
    "vif_df = pd.DataFrame(columns = [\"predictor\", \"VIF\"])\n",
    "\n",
    "# Convert X_train to a DataFrame\n",
    "X_train_df = pd.DataFrame(X_train_transformed, columns = num_cols)\n",
    "\n",
    "# Calculate the VIF for each feature\n",
    "for i in range(len(num_cols)):\n",
    "    vif = variance_inflation_factor(X_train_df[num_cols].values, i)\n",
    "    col = num_cols[i]\n",
    "    vif_df.loc[i] = [col, vif]\n",
    "\n",
    "# Print the VIF dataframe\n",
    "print(vif_df)\n",
    "# Wheelbase and length variables have strong VIF of more than 13 (both)\n",
    "#       wheelbase will be eliminated for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "############################### DECISION TREE ##################################\n",
    "################################################################################\n",
    "\n",
    "# Train set\n",
    "X_train = strat_train_set.copy()\n",
    "y_train = strat_train_set[\"price\"].copy()\n",
    "# Test set\n",
    "X_test = strat_test_set.copy()\n",
    "y_test = strat_test_set[\"price\"].copy()\n",
    "\n",
    "# Rewrite the pipeline\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols)\n",
    "])\n",
    "\n",
    "# Transform the training and test sets\n",
    "X_train_transformed = full_pipeline.fit_transform(X_train)\n",
    "X_test_transformed = full_pipeline.transform(X_test)\n",
    "\n",
    "# Import DecisionTreeRegressor function\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# Define the decision tree regression model\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "# Fit the pipeline to the training data\n",
    "tree_reg.fit(X_train_transformed, y_train)\n",
    "# Predictions\n",
    "tree_y_pred = tree_reg.predict(X_test_transformed)\n",
    "\n",
    "# R2, RMSE, MAE\n",
    "print(\"R2 score Decision Tree model: \", r2_score(y_test, tree_y_pred))\n",
    "print(\"RMSE Decision Tree model: \", np.sqrt(mean_squared_error(y_test, tree_y_pred)))\n",
    "print(\"MAE Decision Tree model: \", mean_absolute_error(y_test, tree_y_pred))\n",
    "\n",
    "# Perform 10-fold cross-validation on the decision tree model\n",
    "scores = cross_val_score(tree_reg, X_train_transformed, y_train, \\\n",
    "                         cv = 10, scoring = \"neg_mean_squared_error\")\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "# Define display cv scores\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "# Print the cross-validation scores and their mean\n",
    "display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "########################## RANDOM FOREST REGRESSOR #############################\n",
    "################################################################################\n",
    "\n",
    "# Train set\n",
    "X_train = strat_train_set.copy()\n",
    "y_train = strat_train_set[\"price\"].copy()\n",
    "# Test set\n",
    "X_test = strat_test_set.copy()\n",
    "y_test = strat_test_set[\"price\"].copy()\n",
    "\n",
    "# Rewrite the pipeline\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols)\n",
    "])\n",
    "\n",
    "# Transform the training and test sets\n",
    "X_train_transformed = full_pipeline.fit_transform(X_train)\n",
    "X_test_transformed = full_pipeline.transform(X_test)\n",
    "\n",
    "# Import random forest regressor function\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(X_train_transformed, y_train)\n",
    "forest_y_pred = tree_reg.predict(X_test_transformed)\n",
    "\n",
    "# R2, RMSE, MAE\n",
    "print(\"R2 score Forest Tree model: \", r2_score(y_test, forest_y_pred))\n",
    "print(\"RMSE Forest Tree model: \", np.sqrt(mean_squared_error(y_test, forest_y_pred)))\n",
    "print(\"MAE Forest Tree model: \", mean_absolute_error(y_test, forest_y_pred))\n",
    "\n",
    "# Perform 10-fold cross-validation on the forest tree model\n",
    "scores = cross_val_score(tree_reg, X_train_transformed, y_train,\n",
    "                             scoring = \"neg_mean_squared_error\", cv = 10)\n",
    "forest_rmse_scores = np.sqrt(-scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "##################### FINE TURE RANDOM FOREST REGRESSOR ########################\n",
    "################################################################################\n",
    "\n",
    "# Train set\n",
    "X_train = strat_train_set.copy()\n",
    "y_train = strat_train_set[\"price\"].copy()\n",
    "# Test set\n",
    "X_test = strat_test_set.copy()\n",
    "y_test = strat_test_set[\"price\"].copy()\n",
    "\n",
    "# Rewrite the pipeline\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols)\n",
    "])\n",
    "\n",
    "# Transform the training and test sets\n",
    "X_train_transformed = full_pipeline.fit_transform(X_train)\n",
    "X_test_transformed = full_pipeline.transform(X_test)\n",
    "\n",
    "\n",
    "############################### GRID SEARCH #####################################\n",
    "\n",
    "# Set parameter grids\n",
    "param_grid = [\n",
    "    {\"n_estimators\": [3, 10, 30], \"max_features\": [2, 4, 6, 8]},\n",
    "    {\"bootstrap\": [False], \"n_estimators\": [3, 10], \"max_features\": [2, 3, 4]},\n",
    "]\n",
    "# Build grid search with parameters\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv = 5,\n",
    "                           scoring = \"neg_mean_squared_error\",\n",
    "                           return_train_score = True)\n",
    "grid_search.fit(X_train_transformed, y_train)\n",
    "# Get the best parameters\n",
    "grid_search.best_params_\n",
    "\n",
    "################################### EVALUATION ##################################\n",
    "\n",
    "# Get the CV results\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)\n",
    "\n",
    "# It's performing a lot poorer than the initial model (though the mean value \n",
    "#    probably fits within the standard deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################# FINAL MODEL ##################################\n",
    "################################################################################\n",
    "\n",
    "# Train set\n",
    "X_train = strat_train_set.copy()\n",
    "X_train.drop(\"wheelbase\", axis = 1, inplace = True)\n",
    "y_train = strat_train_set[\"price\"].copy()\n",
    "# Test set\n",
    "X_test = strat_test_set.copy()\n",
    "X_test.drop(\"wheelbase\", axis = 1, inplace = True)\n",
    "y_test = strat_test_set[\"price\"].copy()\n",
    "\n",
    "# Vector of final variable names\n",
    "final_cols = X_train.select_dtypes(include = np.number).columns.tolist()\n",
    "\n",
    "# Rewrite the pipeline\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, final_cols)\n",
    "])\n",
    "\n",
    "# Transform the training and test sets\n",
    "X_train_transformed = full_pipeline.fit_transform(X_train)\n",
    "X_test_transformed = full_pipeline.transform(X_test)\n",
    "\n",
    "# Import Lasso function \n",
    "from sklearn.linear_model import Lasso\n",
    "# Define the Lasso regression model\n",
    "lasso_reg = Lasso()\n",
    "\n",
    "################################ GRID SEARCH ##################################\n",
    "\n",
    "# Grid values\n",
    "param_grid = {\"alpha\": np.arange(0.001, 1.02, 0.002)}\n",
    "grid_search = GridSearchCV(lasso_reg, param_grid, cv = 10)\n",
    "# Fit the grid\n",
    "grid_search.fit(X_train_transformed, y_train)\n",
    "# Print the best alpha value\n",
    "print(\"Best alpha value: \", grid_search.best_params_[\"alpha\"])\n",
    "\n",
    "############################### FIT BEST ALPHA #################################\n",
    "\n",
    "# Define the Lasso regression model\n",
    "lasso_reg = Lasso(alpha = grid_search.best_params_[\"alpha\"])\n",
    "# Fit the pipeline to the training data\n",
    "lasso_reg.fit(X_train_transformed, y_train)\n",
    "# Lasso model predictions\n",
    "lasso_y_pred = lasso_reg.predict(X_test_transformed)\n",
    "\n",
    "############################# EVALUATE THE MODEL ###############################\n",
    "\n",
    "# R2, RMSE, MAE\n",
    "print(\"R2 score Lasso model: \", r2_score(y_test, lasso_y_pred))\n",
    "print(\"RMSE Lasso model: \", np.sqrt(mean_squared_error(y_test, lasso_y_pred)))\n",
    "print(\"MAE Lasso model: \", mean_absolute_error(y_test, lasso_y_pred))\n",
    "\n",
    "# Perform 10-fold cross-validation on the linear regression model\n",
    "scores = cross_val_score(lasso_reg, X_train_transformed, y_train, \\\n",
    "                         cv = 10, scoring = \"neg_mean_squared_error\")\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "# Define display cv scores\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "# Print the cross-validation scores and their mean\n",
    "display_scores(rmse_scores)\n",
    "\n",
    "# AIC\n",
    "# Get the number of features in the model\n",
    "num_feat = X_train_transformed.shape[1]\n",
    "# Calculate the residual sum of squares\n",
    "rss = np.sum((y_train - lasso_reg.predict(X_train_transformed)) ** 2)\n",
    "# Calculate the AIC score\n",
    "aic = 2 * num_feat + len(y_train) * np.log(rss / len(y_train))\n",
    "# Print AIC score\n",
    "print(\"AIC score Lasso model: \", aic)\n",
    "\n",
    "### VIFs\n",
    "\n",
    "# Create a dataframe to store the VIFs \n",
    "vif_df = pd.DataFrame(columns = [\"predictor\", \"VIF\"])\n",
    "\n",
    "# Convert X_train to a DataFrame\n",
    "X_train_df = pd.DataFrame(X_train_transformed, columns = final_cols)\n",
    "\n",
    "# Calculate the VIF for each feature\n",
    "for i in range(len(final_cols)):\n",
    "    vif = variance_inflation_factor(X_train_df[final_cols].values, i)\n",
    "    col = final_cols[i]\n",
    "    vif_df.loc[i] = [col, vif]\n",
    "\n",
    "# Print the VIF dataframe\n",
    "print(vif_df)\n",
    "# Wheelbase and length variables have strong VIF of more than 13 (both)\n",
    "#       wheelbase will be eliminated for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "############################ FINAL MODEL EVALUATION ############################\n",
    "################################################################################\n",
    "\n",
    "### COEFFICIENTS\n",
    "\n",
    "# Get feature importances\n",
    "coef_abs = np.abs(lasso_reg.coef_)\n",
    "feature_importances = coef_abs / np.sum(coef_abs)\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_features = sorted(zip(feature_importances, final_cols), reverse = True)\n",
    "print(sorted_features)\n",
    "\n",
    "# Most of the variable coefficients come down to 0, which is concerning\n",
    "\n",
    "### RMSE CIs\n",
    "\n",
    "# Get the confidence intervals of RMSE\n",
    "from scipy import stats\n",
    "confidence = 0.95\n",
    "squared_errors = (lasso_y_pred - y_test) ** 2\n",
    "np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
    "loc = squared_errors.mean(),\n",
    "scale = stats.sem(squared_errors)))\n",
    "\n",
    "############################### VISUAL EVALUATION ##############################\n",
    "\n",
    "### Observed vs actual prices\n",
    "\n",
    "# Plot the predicted values against the actual values of test set\n",
    "plt.scatter(y_test, lasso_y_pred)\n",
    "plt.xlabel(\"Actual Prices\")\n",
    "plt.ylabel(\"Predicted Prices\")\n",
    "plt.title(\"Actual vs. Predicted Prices\")\n",
    "# Add a line representing the perfect prediction line\n",
    "x = np.linspace(min(y_test), max(y_test), 100)\n",
    "plt.plot(x, x, color = \"red\")\n",
    "\n",
    "### Residuals goodness-of-fit\n",
    "\n",
    "# Create a dataframe with the predicted and original values\n",
    "residuals_df = pd.DataFrame({\"y_test\": y_test, \"lasso_y_pred\": lasso_y_pred})\n",
    "# Add a column for the residuals\n",
    "residuals_df[\"residuals\"] = residuals_df[\"y_test\"] - residuals_df[\"lasso_y_pred\"]\n",
    "# Define plot colors\n",
    "scatter_color = \"#2c7fb8\"\n",
    "line_color = \"#d95f0e\"\n",
    "# Plot the residuals with customized colors and labels\n",
    "sns.residplot(x = \"lasso_y_pred\", y = \"residuals\", data = residuals_df, lowess = True, \n",
    "              scatter_kws = {\"color\": scatter_color}, line_kws={\"color\": line_color})\n",
    "plt.xlabel(\"Predicted Price\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b813af3179b90fac26e0cdce4b0c4b44496125a0715e29f2997305c8ddbb1ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
